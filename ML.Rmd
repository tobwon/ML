---
title: "HarvardX Data Science Capstone Project - Movielens"
author: "Toby Wong"
date: "May 2021"
output: pdf_document
fontsize: 11pt
sansfont: Arial
---

```{r setup, include=FALSE, warning=FALSE, message=FALSE}
knitr::opts_chunk$set(echo = TRUE)
options(scipen = 100)
```

# 1. Introduction

This report is part of the capstone project for Harvardx Data Science Professional Certificate.

The objective of this project is to compare the performances of movie rating prediction from different machine learning algorithms and identify the best machine learning algorithm model which generates a residual mean squared error (RMSE) score below the target score of `r target<-0.86490
target`.

```{r}
#Target RMSE
target<-0.86490
```


RMSE is computed by using the formula shown below:

$$RMSE=\sqrt{\frac{1}{N}\sum_{}(y_{t}-\hat{y}_{p})^2}$$

Being:\
$N$ = number of samples\
$\hat{y}_{p}$ = predicted value\
$y_{t}$ = target value\

```{r}
# RMSE formula
RMSE <- function(true_ratings, predicted_ratings){
    sqrt(mean((true_ratings - predicted_ratings)^2))
  }
```


This report contains the following sections:

1. Introduction
2. Movielens Data-set
3. Data Exploration
4. Training set and Test Set
5. Models
6. Results
7. Conclusion
8. Reference


# 2. Movielens Data-set

The machine learning algorithms in this project used 10M version of the Movielens data-set (https://grouplens.org/datasets/movielens/10m/) which contains the past rating of movies given by different users.  The following code is supplied by the course and it downloads the Movielens data-set and splitting it into the two data-sets below:

1. "edx" data-set (90% of the original data-set)
2. "validation" data-set (10% of the original data-set)

```{r warning=FALSE, message=FALSE}
###########################################################
# Create edx set, validation set (final hold-out test set)#
###########################################################

# Note: this process could take a couple of minutes

if(!require(tidyverse)) install.packages("tidyverse", repos = "http://cran.us.r-project.org")
if(!require(caret)) install.packages("caret", repos = "http://cran.us.r-project.org")
if(!require(data.table)) install.packages("data.table", repos = "http://cran.us.r-project.org")

library(tidyverse)
library(caret)
library(data.table)

# MovieLens 10M dataset:
# https://grouplens.org/datasets/movielens/10m/
# http://files.grouplens.org/datasets/movielens/ml-10m.zip

dl <- tempfile()
download.file("http://files.grouplens.org/datasets/movielens/ml-10m.zip", dl)

ratings <- fread(text = gsub("::", "\t", readLines(unzip(dl, "ml-10M100K/ratings.dat"))),
                 col.names = c("userId", "movieId", "rating", "timestamp"))

movies <- str_split_fixed(readLines(unzip(dl, "ml-10M100K/movies.dat")), "\\::", 3)
colnames(movies) <- c("movieId", "title", "genres")

# if using R 3.6 or earlier:
movies <- as.data.frame(movies) %>% mutate(movieId = as.numeric(levels(movieId))[movieId],
                                            title = as.character(title),
                                            genres = as.character(genres))
# if using R 4.0 or later:
movies <- as.data.frame(movies) %>% mutate(movieId = as.numeric(movieId),
                                            title = as.character(title),
                                            genres = as.character(genres))


movielens <- left_join(ratings, movies, by = "movieId")

# Validation set will be 10% of MovieLens data
set.seed(1, sample.kind="Rounding") # if using R 3.5 or earlier, use `set.seed(1)`
test_index <- createDataPartition(y = movielens$rating, times = 1, p = 0.1, list = FALSE)
edx <- movielens[-test_index,]
temp <- movielens[test_index,]

# Make sure userId and movieId in validation set are also in edx set
validation <- temp %>% 
      semi_join(edx, by = "movieId") %>%
      semi_join(edx, by = "userId")

# Add rows removed from validation set back into edx set
removed <- anti_join(temp, validation)
edx <- rbind(edx, removed)

rm(dl, ratings, movies, test_index, temp, movielens, removed)
```


# 3. Data Exploration

Before  building the machine learning algorithm models, data exploration is carried out on "edx" data-set to understand the structure and the data it contained.  This will help to determine how the models will be built.

```{r}
# Summary of "edx" data-set
summary(edx)
```

From the summary above, "edx" data-set is noted to be a `r class(edx)` and it contains `r ncol(edx)` columns/variables and `r nrow(edx)` rows/observations.  The columns in "edx" are "userID", "movieID", "rating", "timestamp", "title" and "genres".

```{r}
# Display the first 5 rows of "edx"
head(edx)
```

The information above shown that "userID" and "timestamp" are in integer class, "movieID" and "rating" are in numeric class while "title" and "genres"  are in character class.  After observed the first few rows, it is found the data in "genres" column is combinations of genres and "timestamp" will need to be converted into a suitable format in order for it to be useful in the algorithms.  In addition, "title" column contains both the movie title as well as the year the movie was released/premiered.

```{r}
# Check the genres in "edx"
head(edx$genres)

```

There are `r n_distinct(edx$movieId)` different movies and `r n_distinct(edx$userId)` different users in "edx".  There are in total `n_distinct(edx$genres)` different genres and the high number of different genres is due to there are more than one genre or combination of genres assigned to the each of the movies.

```{r}
# Separate the genres in "edx"
edx_s_g<-edx%>%separate_rows(genres, sep = "\\|")

# Summarised genres in a table
edx_s_g%>%group_by(genres) %>%
  summarize(count = n(), .groups='drop') %>%
  arrange(desc(count))
```

Once the genres are separated, there are only 20 distinct genres and the genre rated the most is "Drama" with 3910127 counts.


```{r}
# Plot the total count of each rating
edx %>% ggplot(aes(rating)) +
  geom_histogram(binwidth = 0.25, fill = "black")+
  xlab("Rating")+
  ylab("Number of Rating")
```

The plot above shown the distribution of the ratings and it is observed that the most of the rating given is 4 and in general half star ratings are less common than whole star ratings.


```{r}
# Display the top 10 movies with hightest number of rating 
edx %>% group_by(movieId, title) %>%
	summarize(count = n(), .groups='drop') %>%
	arrange(desc(count))
```

The above table shown the top 10 most rated movies and the movie which rated the most is "Pulp Fiction" with 31362 counts.

```{r}
# plot the number of times movies were rated
edx %>% 
  count(movieId) %>% 
  ggplot(aes(n)) + 
  geom_histogram(bins=500, color = "black") +
  xlab("Number of Ratings per Movie")+
  ylab("Number of Movie")+
  scale_x_log10() + 
  ggtitle("Count of Rating per Movie")
```

From the above histogram shown the number of movie against the number of rating and there is a wide range in how many times a movie is rated as a few of the movies rated more than 10000 times but around 125 movies rated only one time.


# 4. Training set and Test Set

Before the modelling of the algorithms, the `edx` data-set is split into "training_set" and "test_set".  "training_set" is 90% of "edx" while "test_set" is 10% of "edx".  "training_set" will be used to build the models and then using "test_set" set to test and generate RMSE scores.

After the models are developed and tested, "edx" and "validation" data-sets will be used to train and then validate the final model(s) to verify if it can successfully achieve a score less than the target RMSE.

```{r}
###################################
# Generate train_set and test_set #
###################################

test_index <- createDataPartition(y = edx$rating, times = 1, p = 0.1, list = FALSE)
train_set <- edx[-test_index,]
test_set <- edx[test_index,]

# Make sure userId and movieId in test set are also in train set
test_set <- test_set %>% 
  semi_join(train_set, by = "movieId") %>%
  semi_join(train_set, by = "userId")

rm(test_index)
```


# 5. Models
A series of machine learning models using linear regression were developed progressively by adding and accumulating the effects of the variables from the data-set.  After all the effects of the variables are included in the model, regularization will be applied to the final model to enhance the performance.  


## 5.1 Model 1 Average of All Ratings

The first model is the simplest one and it calculated the average of the ratings while  disregarded all other variables.  The resulted RMSE for Model 1 is used as a benchmark for the other models.

```{r}
##################################
# Model 1 Average of All Ratings #
##################################

mu<-mean(train_set$rating)
model1 <- mu

# RMSE calculation 
rmse1 <- RMSE(test_set$rating, model1)

```

The resulted RMSE score for Model 1 is `r rmse1`.


## 5.2 Model 2 Average with Movie Effect

The second model is built on top of Model 1 by taking into account the effect of the movies (movieID) by using the average rating of the movies.

```{r}
#####################################
# Model 2 Average with Movie Effect #
#####################################

b_i <- train_set %>% 
  group_by(movieId) %>% 
  summarize(b_i = mean(rating - mu), .groups='drop')

# Prediction
model2 <- mu + test_set %>% 
  left_join(b_i, by='movieId') %>%
  pull(b_i)

# RMSE calculation 
rmse2<-RMSE(test_set$rating, model2)

```

Model 2 achieved a RMSE score of `r rmse2` and it improved from Model 1.


## 5.3 Model 3 Average with Movie and User Effects

The third model combined average rating, movie effect and user effect used in the previous model and added the average rating of individual users (userID).

```{r}
###############################################
# Model 3 Average with Movie and User Effects #
###############################################

b_u <- train_set %>% 
  left_join(b_i, by='movieId') %>%
  group_by(userId) %>%
  summarize(b_u = mean(rating - mu - b_i), .groups='drop')

# Prediction
model3 <- test_set %>% 
  left_join(b_i, by='movieId') %>%
  left_join(b_u, by='userId') %>%
  mutate(pred = mu + b_i + b_u) %>%
  pull(pred)

# RMSE calculation 
rmse3 <- RMSE(test_set$rating, model3)

```

The RMSE score for Model 3 is `r rmse3` and the model made good improvement over model 2. 


## 5.4 Model 4 Average with Movie, User and Genre Effects

Model 4 built on Model 3 by adding the effect of the movie genres (genres).  As shown during data exploration, there are `r n_distinct(edx$genres)` different combinations of genres and this model includes the effect of the average rating of these combinations to try to improve the prediction. 

```{r}
######################################################
# Model 4 Average with Movie, User and Genre Effects #
######################################################

b_g <- train_set %>% 
  left_join(b_i, by='movieId') %>%
  left_join(b_u, by='userId') %>%
  group_by(genres) %>%
  summarize(b_g = mean(rating - mu - b_i - b_u), .groups='drop')

# Prediction
model4 <- test_set %>% 
  left_join(b_i, by='movieId') %>%
  left_join(b_u, by='userId') %>%
  left_join(b_g, by="genres") %>%
  mutate(pred = mu + b_i + b_u + b_g) %>%
  pull(pred)

# RMSE calculation 
rmse4 <- RMSE(test_set$rating, model4)

```

Model 4 RMSE score is `r rmse4` and it shown combinations of genres can only improved the prediction slightly as the improvement from last model is insignificant.


## 5.5 Model 5 Average with Movie, User, Genre and Time Effects

Model 5 included the effect of time (timestamp) when the movie is rated in addition to the movie, user and genre effects from Model 4.  As noted during data exploration, "timestamp" data needs to be converted to useful format and it is converted into the week when the movie was rated by using "lubridate" package.

```{r warning=FALSE, message=FALSE}
############################################################
# Model 5 Average with Movie, User, Genre and Time Effects #
############################################################

#Load lubridate to convert timestamp to date
if(!require(lubridate)) install.packages("data.table", repos = "http://cran.us.r-project.org")
library(lubridate)

b_t <- train_set %>% 
  left_join(b_i, by='movieId') %>%
  left_join(b_u, by='userId') %>%
  left_join(b_g, by='genres') %>%
  mutate(date = round_date(as_datetime(timestamp), unit = "week"))%>%
  group_by(date) %>%
  summarize(b_t = mean(rating - mu - b_i - b_u - b_g), .groups='drop')

# Prediction
model5 <- test_set %>%
  left_join(b_i, by='movieId') %>%
  left_join(b_u, by='userId') %>%
  left_join(b_g, by="genres") %>%
  mutate(date = round_date(as_datetime(timestamp), unit = "week"))%>%
  left_join(b_t, by='date')%>%
  mutate(pred = mu + b_i + b_u + b_g + b_t) %>%
  pull(pred)

# RMSE calculation 
rmse5 <- RMSE(test_set$rating, model5)
```

Model 5 achieved a RMSE score of `r rmse5` and the improvement of time effect is also proved to be insignificant.


## 5.6 Model 6 Average with Movie, User, Genre, Time and Premiere Year Effects

Further to the time when the movie was rated, the year the movie was released/premiered may also contributed to how it was rated.  Model 6 included the effect of the premiere year by averaging the rating according to the premiere year.

```{r}
###########################################################################
# Model 6 Average with Movie, User, Genre, Time and premiere Year Effects #
###########################################################################

b_y <- train_set %>% 
  left_join(b_i, by='movieId') %>%
  left_join(b_u, by='userId') %>%
  left_join(b_g, by='genres') %>%
  mutate(date = round_date(as_datetime(timestamp), unit = "week"))%>%
  left_join(b_t, by='date') %>%
  mutate(premiere = as.numeric(str_sub(title,-5,-2))) %>%
  group_by(premiere) %>%
  summarize(b_y = mean(rating - mu - b_i - b_u - b_g - b_t), .groups='drop')

# Prediction
model6 <- test_set %>% 
  left_join(b_i, by='movieId') %>%
  left_join(b_u, by='userId') %>%
  left_join(b_g, by="genres") %>%
  mutate(date = round_date(as_datetime(timestamp), unit = "week"))%>%
  left_join(b_t, by='date')%>%
  mutate(premiere = as.numeric(str_sub(title,-5,-2))) %>%
  left_join(b_y, by="premiere")%>%
  mutate(pred = mu + b_i + b_u + b_g + b_t + b_y) %>%
  pull(pred)

# RMSE calculation 
rmse6 <- RMSE(test_set$rating, model6)

```

The RMSE score generated by Model 6 is `r rmse6` and it shown movie premiere year also only had small effect on the prediction and the target RMSE score could not be met despite all the variables had included in the model.


## 5.7 Model 7 Regularized Movie, User, Genre, Time and Premiere Year Effects

Data exploration shown some of the movies were rated very few times, and the same can also applied to user, genre, time and premiere year.  The accuracy of the prediction is affected by these small number of ratings and therefore the performance can be improved by penalizing the data with few ratings through regularization.  Model 7 computes the prediction using a penalty term in a regularized model.

```{r}
##########################################################################
# Model 7 Regularized Movie, User, Genre, Time and Premiere Year Effects #
##########################################################################

# Finding the optimum tuning value through cross validation
lambdas <- seq(4, 6, 0.25)

# For each lambda, find b_i, b_u, b_g, b_t and b_y followed by rating prediction
rmses1 <- sapply(lambdas, function(l){
  
  b_i <- train_set %>% 
    group_by(movieId) %>%
    summarize(b_i = sum(rating - mu)/(n()+l), .groups='drop')
  
  b_u <- train_set %>% 
    left_join(b_i, by="movieId") %>%
    group_by(userId) %>%
    summarize(b_u = sum(rating - mu - b_i)/(n()+l), .groups='drop')
  
  b_g <- train_set %>%
    left_join(b_i, by="movieId") %>%
    left_join(b_u, by="userId") %>%
    group_by(genres) %>%
    summarize(b_g = sum(rating - mu - b_i - b_u)/(n()+l), .groups='drop')
  
  b_t <- train_set %>% 
    left_join(b_i, by='movieId') %>%
    left_join(b_u, by='userId') %>%
    left_join(b_g, by='genres') %>%
    mutate(date = round_date(as_datetime(timestamp), unit = "week"))%>%
    group_by(date) %>%
    summarize(b_t = mean(rating - mu - b_i - b_u - b_g)/(n()+l), .groups='drop')
  
  b_y <- train_set %>%
    left_join(b_i, by='movieId') %>%
    left_join(b_u, by='userId') %>%
    left_join(b_g, by='genres') %>%
    mutate(date = round_date(as_datetime(timestamp), unit = "week"))%>%
    left_join(b_t, by='date') %>%
    mutate(premiere = as.numeric(str_sub(title,-5,-2))) %>%
    group_by(premiere) %>%
    summarize(b_y = mean(rating - mu - b_i - b_u - b_g - b_t)/(n()+l), .groups='drop')

  predicted_ratings <- test_set %>%
    left_join(b_i, by = "movieId") %>%
    left_join(b_u, by = "userId") %>%
    left_join(b_g, by = "genres") %>%
    mutate(date = round_date(as_datetime(timestamp), unit = "week"))%>%
    left_join(b_t, by='date')%>%
    mutate(premiere = as.numeric(str_sub(title,-5,-2))) %>%
    left_join(b_y, by="premiere")%>%
    mutate(pred = mu + b_i + b_u + b_g + b_t + b_y) %>%
    pull(pred)

    return(RMSE(test_set$rating, predicted_ratings))
})

lambda<-lambdas[which.min(rmses1)]

# Movie Effect
b_i <- train_set %>% 
  group_by(movieId) %>%
  summarize(b_i = sum(rating - mu)/(n()+lambda), .groups='drop')

# User Effect
b_u <- train_set %>% 
  left_join(b_i, by="movieId") %>%
  group_by(userId) %>%
  summarize(b_u = sum(rating - b_i - mu)/(n()+lambda), .groups='drop')

# Genre Effect
b_g <- train_set %>%
  left_join(b_i, by="movieId") %>%
  left_join(b_u, by="userId") %>%
  group_by(genres) %>%
  summarize(b_g = sum(rating - b_i - b_u - mu)/(n()+lambda), .groups='drop')

# Time Effect
b_t <- train_set %>% 
  left_join(b_i, by='movieId') %>%
  left_join(b_u, by='userId') %>%
  left_join(b_g, by='genres') %>%
  mutate(date = round_date(as_datetime(timestamp), unit = "week"))%>%
  group_by(date) %>%
  summarize(b_t = mean(rating - mu - b_i - b_u - b_g)/(n()+lambda), .groups='drop')

# premiere Year Effect
b_y <- train_set %>%
  left_join(b_i, by='movieId') %>%
  left_join(b_u, by='userId') %>%
  left_join(b_g, by='genres') %>%
  mutate(date = round_date(as_datetime(timestamp), unit = "week"))%>%
  left_join(b_t, by='date') %>%
  mutate(premiere = as.numeric(str_sub(title,-5,-2))) %>%
  group_by(premiere) %>%
  summarize(b_y = mean(rating - mu - b_i - b_u - b_g - b_t)/(n()+lambda), .groups='drop')

# Prediction
model7 <- test_set %>% 
  left_join(b_i, by = "movieId") %>%
  left_join(b_u, by = "userId") %>%
  left_join(b_g, by = "genres") %>%
  mutate(date = round_date(as_datetime(timestamp), unit = "week"))%>%
  left_join(b_t, by='date')%>%
  mutate(premiere = as.numeric(str_sub(title,-5,-2))) %>%
  left_join(b_y, by="premiere")%>%
  mutate(pred = mu + b_i + b_u + b_g + b_t + b_y) %>%
  pull(pred)

# RMSE calculation 
rmse7 <- RMSE(test_set$rating, model7)

```

Model 7 achieved a RMSE score of `r rmse7` and the score did not improve meaningfuly with regularization and it is still not meeting the target RMSE score.


## 5.8 Model 8 Regularized Movie, User, Separated Genre, Time and Premiere Year Effects
As observed during data exploration, the genres of the movies were combined in the data-set and this affected the accuracy of the prediction as it resulted in large number of different combinations rather than the smaller number of genres.  Model 8 built from the regularized  effects incorporated in Model 7 while separated the genres to improve the RMSE score. 

```{r}
####################################################################################
# Model 8 Regularized Movie, User, Separated Genre, Time and Premiere Year Effects #
####################################################################################

# Separate the genres in "test_set" and "train_set"
test_set_s_g<-test_set%>%separate_rows(genres, sep = "\\|")
train_set_s_g<-train_set%>%separate_rows(genres, sep = "\\|")

# Finding the optimum tuning value through cross validation
lambdas2 <- seq(12, 14, 0.25)
 
# Calculate the average rating after separarted the genres 
mu2 <- mean(train_set_s_g$rating)

# For each lambda, find b_i, b_u, b_g, b_t and b_y followed by rating prediction
rmses2 <- sapply(lambdas2, function(l){
  
  b_i2 <- train_set_s_g %>% 
    group_by(movieId) %>%
    summarize(b_i2 = sum(rating - mu2)/(n()+l), .groups='drop')
  
  b_u2 <- train_set_s_g %>% 
    left_join(b_i2, by="movieId") %>%
    group_by(userId) %>%
    summarize(b_u2 = sum(rating - mu2 - b_i2)/(n()+l), .groups='drop')
  
  b_g2 <- train_set_s_g %>%
    left_join(b_i2, by="movieId") %>%
    left_join(b_u2, by="userId") %>%
    group_by(genres) %>%
    summarize(b_g2 = sum(rating - mu2 - b_i2 - b_u2)/(n()+l), .groups='drop')
  
  b_t2 <- train_set_s_g %>% 
    left_join(b_i2, by='movieId') %>%
    left_join(b_u2, by='userId') %>%
    left_join(b_g2, by='genres') %>%
    mutate(date = round_date(as_datetime(timestamp), unit = "week"))%>%
    group_by(date) %>%
    summarize(b_t2 = mean(rating - mu2 - b_i2 - b_u2 - b_g2)/(n()+l), .groups='drop')
  
  b_y2 <- train_set_s_g %>%
    left_join(b_i2, by='movieId') %>%
    left_join(b_u2, by='userId') %>%
    left_join(b_g2, by='genres') %>%
    mutate(date = round_date(as_datetime(timestamp), unit = "week"))%>%
    left_join(b_t2, by='date') %>%
    mutate(premiere = as.numeric(str_sub(title,-5,-2))) %>%
    group_by(premiere) %>%
    summarize(b_y2 = mean(rating - mu2 - b_i2 - b_u2 - b_g2 - b_t2)/(n()+l), .groups='drop')

  predicted_ratings <- test_set_s_g %>%
    left_join(b_i2, by = "movieId") %>%
    left_join(b_u2, by = "userId") %>%
    left_join(b_g2, by = "genres") %>%
    mutate(date = round_date(as_datetime(timestamp), unit = "week"))%>%
    left_join(b_t2, by='date')%>%
    mutate(premiere = as.numeric(str_sub(title,-5,-2))) %>%
    left_join(b_y2, by="premiere")%>%
    mutate(pred = mu2 + b_i2 + b_u2 + b_g2 + b_t2 + b_y2) %>%
    pull(pred)

    return(RMSE(test_set_s_g$rating, predicted_ratings))
})

lambda2<-lambdas2[which.min(rmses2)]

# Movie Effect
b_i2 <- train_set_s_g %>% 
  group_by(movieId) %>%
  summarize(b_i2 = sum(rating - mu2)/(n()+lambda2), .groups='drop')

# User Effect
b_u2 <- train_set_s_g %>% 
  left_join(b_i2, by="movieId") %>%
  group_by(userId) %>%
  summarize(b_u2 = sum(rating - b_i2 - mu2)/(n()+lambda2), .groups='drop')

# Genre Effect
b_g2 <- train_set_s_g %>%
  left_join(b_i2, by="movieId") %>%
  left_join(b_u2, by="userId") %>%
  group_by(genres) %>%
  summarize(b_g2 = sum(rating - b_i2 - b_u2 - mu2)/(n()+lambda2), .groups='drop')

# Time Effect
b_t2 <- train_set_s_g %>% 
  left_join(b_i2, by='movieId') %>%
  left_join(b_u2, by='userId') %>%
  left_join(b_g2, by='genres') %>%
  mutate(date = round_date(as_datetime(timestamp), unit = "week"))%>%
  group_by(date) %>%
  summarize(b_t2 = mean(rating - mu2 - b_i2 - b_u2 - b_g2)/(n()+lambda2), .groups='drop')

# premiere Year Effect
b_y2 <- train_set_s_g %>%
  left_join(b_i2, by='movieId') %>%
  left_join(b_u2, by='userId') %>%
  left_join(b_g2, by='genres') %>%
  mutate(date = round_date(as_datetime(timestamp), unit = "week"))%>%
  left_join(b_t2, by='date') %>%
  mutate(premiere = as.numeric(str_sub(title,-5,-2))) %>%
  group_by(premiere) %>%
  summarize(b_y2 = mean(rating - mu2 - b_i2 - b_u2 - b_g2 - b_t2)/(n()+lambda2), .groups='drop')

# Prediction
model8 <- test_set_s_g %>% 
  left_join(b_i2, by = "movieId") %>%
  left_join(b_u2, by = "userId") %>%
  left_join(b_g2, by = "genres") %>%
  mutate(date = round_date(as_datetime(timestamp), unit = "week"))%>%
  left_join(b_t2, by='date')%>%
  mutate(premiere = as.numeric(str_sub(title,-5,-2))) %>%
  left_join(b_y2, by="premiere")%>%
  mutate(pred = mu2 + b_i2 + b_u2 + b_g2 + b_t2 + b_y2) %>%
  pull(pred)

# RMSE calculation 
rmse8 <- RMSE(test_set_s_g$rating, model8)

```

The RMSE score for Model 8 is `r rmse8` and met the target RMSE score.  It proved movie genres once separated can improve the prediction quite significantly.


# 6. Results

## 6.1 Validation

From of the models tested, Model 8 Regularized Movie, User, Separated Genre, Time and Premiere Year produced the best RMSE score of `r rmse8` which met the target RMSE score `r target`.   For the purpose of validation, The two regularized models -  Model 7 and Model 8 will be validated by using `edx` and `validation` data-sets.


## 6.2 Validation 1-Regularized Movie, User, Genre, Time and premiere Year Effects

```{r}
##############################################################################
# Validation 1-Regularized Movie, User, Genre, Time and premiere Year Effects#
##############################################################################

# Calculate the average of the ratings in "edx" data-set
mu_val <- mean(edx$rating)

# Movie Effect
b_i_val <- edx %>% 
  group_by(movieId) %>%
  summarize(b_i_val = sum(rating - mu_val)/(n()+lambda), .groups='drop')

# User Effect
b_u_val <- edx %>% 
  left_join(b_i_val, by="movieId") %>%
  group_by(userId) %>%
  summarize(b_u_val = sum(rating - b_i_val - mu_val)/(n()+lambda), .groups='drop')

# Genre Effect
b_g_val <- edx %>% 
  left_join(b_i_val, by="movieId") %>%
  left_join(b_u_val, by="userId") %>%
  group_by(genres) %>%
  summarize(b_g_val = sum(rating - b_i_val - b_u_val - mu_val)/(n()+lambda), .groups='drop')

# Time Effect
b_t_val <- edx %>%
  left_join(b_i_val, by='movieId') %>%
  left_join(b_u_val, by='userId') %>%
  left_join(b_g_val, by='genres') %>%
  mutate(date = round_date(as_datetime(timestamp), unit = "week"))%>%
  group_by(date) %>%
  summarize(b_t_val = mean(rating - mu_val - b_i_val - b_u_val - b_g_val)/(n()+lambda), .groups='drop')

# premiere Year Effect
b_y_val <- edx %>%
  left_join(b_i_val, by='movieId') %>%
  left_join(b_u_val, by='userId') %>%
  left_join(b_g_val, by='genres') %>%
  mutate(date = round_date(as_datetime(timestamp), unit = "week"))%>%
  left_join(b_t_val, by='date') %>%
  mutate(premiere = as.numeric(str_sub(title,-5,-2))) %>%
  group_by(premiere) %>%
  summarize(b_y_val = mean(rating - mu_val - b_i_val - b_u_val - b_g_val - b_t_val)/(n()+lambda), .groups='drop')

# Prediction
model_val1 <- validation %>% 
  left_join(b_i_val, by = "movieId") %>%
  left_join(b_u_val, by = "userId") %>%
  left_join(b_g_val, by = "genres") %>%
  mutate(date = round_date(as_datetime(timestamp), unit = "week"))%>%
  left_join(b_t_val, by='date')%>%
  mutate(premiere = as.numeric(str_sub(title,-5,-2))) %>%
  left_join(b_y_val, by="premiere")%>%
  mutate(pred = mu_val + b_i_val + b_u_val + b_g_val + b_t_val + b_y_val) %>%
  pull(pred)

# RMSE calculation 
rmse_val1 <- RMSE(validation$rating, model_val1)

```

The resulted RMSE score after validation using using "edx" and "validation" data-sets on Model 7 is `r rmse_val1`.  It is a better result and unexpectedly meeting the target RMSE. 


## 6.3 Validation 2-Regularized Movie, User, Separated Genre, Time and premiere Year Effects

```{r}
########################################################################################
# Validation 2-Regularized Movie, User, Separated Genre, Time and Premiere Year Effects#
########################################################################################

# Separate the genres in "validation" data-set
validation_s_g<-validation%>%separate_rows(genres, sep = "\\|")

# Calculate the average rating after separarted the genres 
mu_val2 <- mean(edx_s_g$rating)

# Movie Effect
b_i_val2 <- edx_s_g %>% 
  group_by(movieId) %>%
  summarize(b_i_val2 = sum(rating - mu_val2)/(n()+lambda2), .groups='drop')

# User Effect
b_u_val2 <- edx_s_g %>% 
  left_join(b_i_val2, by="movieId") %>%
  group_by(userId) %>%
  summarize(b_u_val2 = sum(rating - b_i_val2 - mu_val2)/(n()+lambda2), .groups='drop')

# Genre Effect
b_g_val2 <- edx_s_g %>% 
  left_join(b_i_val2, by="movieId") %>%
  left_join(b_u_val2, by="userId") %>%
  group_by(genres) %>%
  summarize(b_g_val2 = sum(rating - b_i_val2 - b_u_val2 - mu_val2)/(n()+lambda2), .groups='drop')

# Time Effect
b_t_val2 <- edx_s_g %>%
  left_join(b_i_val2, by='movieId') %>%
  left_join(b_u_val2, by='userId') %>%
  left_join(b_g_val2, by='genres') %>%
  mutate(date = round_date(as_datetime(timestamp), unit = "week"))%>%
  group_by(date) %>%
  summarize(b_t_val2 = mean(rating - mu_val2 - b_i_val2 - b_u_val2 - b_g_val2)/(n()+lambda2), .groups='drop')

# premiere Year Effect
b_y_val2 <- edx_s_g%>%
  left_join(b_i_val2, by='movieId') %>%
  left_join(b_u_val2, by='userId') %>%
  left_join(b_g_val2, by='genres') %>%
  mutate(date = round_date(as_datetime(timestamp), unit = "week"))%>%
  left_join(b_t_val2, by='date') %>%
  mutate(premiere = as.numeric(str_sub(title,-5,-2))) %>%
  group_by(premiere) %>%
  summarize(b_y_val2 = mean(rating - mu_val2 - b_i_val2 - b_u_val2 - b_g_val2 - b_t_val2)/(n()+lambda2), .groups='drop')

# Prediction
model_val2 <- validation_s_g %>% 
  left_join(b_i_val2, by = "movieId") %>%
  left_join(b_u_val2, by = "userId") %>%
  left_join(b_g_val2, by = "genres") %>%
  mutate(date = round_date(as_datetime(timestamp), unit = "week"))%>%
  left_join(b_t_val2, by='date')%>%
  mutate(premiere = as.numeric(str_sub(title,-5,-2))) %>%
  left_join(b_y_val2, by="premiere")%>%
  mutate(pred = mu_val2 + b_i_val2 + b_u_val2 + b_g_val2 + b_t_val2 + b_y_val2) %>%
  pull(pred)

# RMSE calculation 
rmse_val2 <- RMSE(validation_s_g$rating, model_val2)

```

Using the "edx" and "validation" data-sets, Model 8 achieved a RMSE score of `r rmse_val2` and it is the best score among all the models and it met the target RMSE as expected.


## 6.4 Summary of Results

The RMSE scores produced by different models are summarized in the table below.

```{r}
#################
# Summary table #
#################

# Prodcue summary table
summary <- tibble(Method = c("Target", "Model 1-Average of All Ratings", "Model 2-Average with Movie Effect", "Model 3-Average with Movie and User Effects", "Model 4-Average with Movie, User and Genre Effects", "Model 5-Average with Movie, User, Genre and Time Effects", "Model 6-Average with Movie, User, Genre, Time and Premiere Year Effects", "Model 7-Regularized Movie, User, Genre, Time and Premiere Year Effects", "Model 8-Regularized Movie, User, Separated Genre, Time and Premiere Year Effects", "Validation 1-Regularized Movie, User, Genre, Time and Premiere Year Effects", "Validation 2-Regularized Movie, User, Separated Genre, Time and Premiere Year Effects"), RMSE = c(target, rmse1, rmse2, rmse3, rmse4, rmse5, rmse6, rmse7, rmse8, rmse_val1, rmse_val2), "Difference to Model 1" = c(rmse1 - target, rmse1 - rmse1, rmse2 - rmse1, rmse3 - rmse1, rmse4 - rmse1, rmse5 - rmse1, rmse6 - rmse1, rmse7 - rmse1, rmse8 - rmse1, rmse_val1 - rmse1, rmse_val2 - rmse1), "Difference to Last Model" = c("NA", rmse1 - target, rmse2 - rmse1, rmse3 - rmse2, rmse4 - rmse3, rmse5 - rmse4, rmse6 - rmse5, rmse7 - rmse6, rmse8 - rmse7, rmse_val1 - rmse8, rmse_val2 - rmse_val1), "Difference to Target" = c(target-target, rmse1 - target, rmse2 - target, rmse3 - target, rmse4 - target, rmse5 - target, rmse6 - target, rmse7 - target, rmse8 - target, rmse_val1 - target, rmse_val2 - target))

summary

```

Both Model 7 and Model 8 successfully validated meeting the target RMSE.  The following observations were noted from the results:

1. The time of review (timestamp) and the year when the movie was released/premiered had very little impact on the rating prediction accuracy.
2. The genre (genres) in the original combined format also had very little impact on the prediction however once the genres were separated, the impact became quite significant.
3. Movie (movieID), and user (userID) made significant impacts to the accuracy of prediction.
4. The two validation models used "edx" and "validation" data-sets provides more accurate predictions and this may due to the larger sample size.


# 7. Conclusion

After testing the different machine learning models, Model 8 Regularized Movie, User, Separated Genre, Time and Premiere Year Effects model achieved the best RMSE score.  The final model is validated by using the "validation" data-set and the resulted a RMSE score of  `r rmse_val2`, successfully passed the target RMSE of `r target` and met the project objective.


# 8. Reference

Rafael A. Irizarry (2019), Introduction to Data Science: Data Analysis and Prediction Algorithms with R